{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529e5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['mean', '50%']\n",
    "\n",
    "data_paths = {\n",
    "        'T1': 'data/dataset/t1',\n",
    "        'T2': 'data/dataset/t2',\n",
    "        'mask': 'data/dataset/mask',\n",
    "        'T1_avaliacao': 'data/avaliacao/t1',\n",
    "        'T2_avaliacao': 'data/avaliacao/t2',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7322c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "class Unet_module(nn.Module):\n",
    "    def __init__(self, kernel_size, de_kernel_size, channel_list, down_up='down'):\n",
    "        super(Unet_module, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_list[0], channel_list[1], kernel_size, 1, (kernel_size - 1) // 2)\n",
    "        self.conv2 = nn.Conv2d(channel_list[1], channel_list[2], kernel_size, 1, (kernel_size - 1) // 2)\n",
    "        self.relu1 = nn.PReLU()\n",
    "        self.relu2 = nn.PReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(channel_list[1])\n",
    "        self.bn2 = nn.BatchNorm2d(channel_list[2])\n",
    "        self.bridge_conv = nn.Conv2d(channel_list[0], channel_list[-1], kernel_size, 1, (kernel_size - 1) // 2)\n",
    "\n",
    "        if down_up == 'down':\n",
    "            self.sample = nn.Sequential(\n",
    "                nn.Conv2d(channel_list[2], channel_list[2], de_kernel_size, 2, (de_kernel_size - 1) // 2, 1),\n",
    "                nn.BatchNorm2d(channel_list[2]), nn.PReLU())\n",
    "        else:\n",
    "            self.sample = nn.Sequential(\n",
    "                nn.ConvTranspose2d(channel_list[2], channel_list[2], de_kernel_size, 2, (de_kernel_size - 1) // 2),\n",
    "                nn.BatchNorm2d(channel_list[2]), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.bridge_conv(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = x + res\n",
    "        next_layer = self.sample(x)\n",
    "\n",
    "        return next_layer, x\n",
    "    \n",
    "class de_conv_module(nn.Module):\n",
    "    def __init__(self, kernel_size, de_kernel_size, channel_list, down_up='down'):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_list[0], channel_list[1], kernel_size, 1, (kernel_size - 1) // 2)\n",
    "        self.conv2 = nn.Conv2d(channel_list[1], channel_list[2], kernel_size, 1, (kernel_size - 1) // 2)\n",
    "        self.relu1 = nn.PReLU()\n",
    "        self.relu2 = nn.PReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(channel_list[1])\n",
    "        self.bn2 = nn.BatchNorm2d(channel_list[2])\n",
    "        self.bridge_conv = nn.Conv2d(channel_list[0], channel_list[-1], kernel_size, 1, (kernel_size - 1) // 2)\n",
    "\n",
    "        if down_up == 'down':\n",
    "            self.sample = nn.Sequential(\n",
    "                nn.Conv2d(channel_list[2], channel_list[2], de_kernel_size, 2, (de_kernel_size - 1) // 2, 1),\n",
    "                nn.BatchNorm2d(channel_list[2]), nn.PReLU())\n",
    "        else:\n",
    "            self.sample = nn.Sequential(\n",
    "                nn.ConvTranspose2d(channel_list[2], channel_list[2], de_kernel_size, 2, (de_kernel_size - 1) // 2),\n",
    "                nn.BatchNorm2d(channel_list[2]), nn.ReLU())\n",
    "\n",
    "    def forward(self, x, x1):\n",
    "        x = th.cat([x, x1], dim=1)\n",
    "        res = self.bridge_conv(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = x + res\n",
    "        next_layer = self.sample(x)\n",
    "\n",
    "        return next_layer\n",
    "\n",
    "class FCN_2D(nn.Module):\n",
    "    def __init__(self, in_channel, layers):\n",
    "        super().__init__()\n",
    "        # channel=2\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channel, layers, 5, 1, padding=2), nn.BatchNorm2d(layers), nn.PReLU())\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(layers, layers * 2, 2, 2, padding=0), nn.BatchNorm2d(layers * 2),\n",
    "                                   nn.PReLU())\n",
    "        self.conv3 = Unet_module(5, 2, [layers * 2, layers * 2, layers * 4], 'down')\n",
    "        self.conv4 = Unet_module(5, 2, [layers * 4, layers * 4, layers * 8], 'down')\n",
    "        self.conv5 = Unet_module(5, 2, [layers * 8, layers * 8, layers * 16], 'down')\n",
    "\n",
    "        self.de_conv1 = Unet_module(5, 2, [layers * 16, layers * 32, layers * 16], down_up='up')\n",
    "        self.de_conv2 = de_conv_module(5, 2, [layers * 32, layers * 8, layers * 8], down_up='up')\n",
    "        self.de_conv3 = de_conv_module(5, 2, [layers * 16, layers * 4, layers * 4], down_up='up')\n",
    "        self.de_conv4 = de_conv_module(5, 2, [layers * 8, layers * 2, layers], down_up='up')\n",
    "\n",
    "        self.last_conv = nn.Conv2d(layers * 2, 1, 1, 1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x_1 = x\n",
    "        x = self.conv2(x)\n",
    "        x, x_2 = self.conv3(x)\n",
    "        x, x_3 = self.conv4(x)\n",
    "        x, x_4 = self.conv5(x)\n",
    "\n",
    "        x, _ = self.de_conv1(x)\n",
    "        x = self.de_conv2(x, x_4)\n",
    "        x = self.de_conv3(x, x_3)\n",
    "        x = self.de_conv4(x, x_2)\n",
    "\n",
    "        x = th.cat([x, x_1], dim=1)\n",
    "        output = self.last_conv(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c80c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import rasterio\n",
    "\n",
    "class WorCapDataset(Dataset):\n",
    "    def __init__(self, T10_dir, T20_dir, mask_dir, ids, transform=None):\n",
    "        self.T10_dir = T10_dir\n",
    "        self.T20_dir = T20_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.ids = ids\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def read_image(self, path):\n",
    "        with rasterio.open(path) as src:\n",
    "            img = src.read().astype(np.float32)\n",
    "            img = np.nan_to_num(img, nan=0.0)\n",
    "            img_min = img.min()\n",
    "            img_max = img.max()\n",
    "            if img_max - img_min > 0:\n",
    "                img = (img - img_min) / (img_max - img_min)\n",
    "            else:\n",
    "                img = np.zeros_like(img)\n",
    "        return torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "    def read_mask(self, path):\n",
    "        with rasterio.open(path) as src:\n",
    "            mask = src.read(1).astype(np.float32)\n",
    "            mask = np.nan_to_num(mask, nan=0.0)\n",
    "            mask = np.where(mask > 0, 1.0, 0.0)\n",
    "        return torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id_ = self.ids[idx]\n",
    "        fname = f\"recorte_{id_}.tif\"\n",
    "        T10_path = os.path.join(self.T10_dir, fname)\n",
    "        T20_path = os.path.join(self.T20_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "\n",
    "        t1 = self.read_image(T10_path)\n",
    "        t2 = self.read_image(T20_path)\n",
    "        mask = self.read_mask(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            t1 = self.transform(t1)\n",
    "            t2 = self.transform(t2)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        T = torch.cat([t1, t2], dim=0)\n",
    "        return T, mask, id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d94a1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spyndex\n",
    "class WorCapDiffDataset(Dataset):\n",
    "    def __init__(self, T10_dir, T20_dir, mask_dir, ids, transform=None):\n",
    "        self.T10_dir = T10_dir\n",
    "        self.T20_dir = T20_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.ids = ids\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def read_image(self, path):\n",
    "        with rasterio.open(path) as src:\n",
    "            S1 = src.read(3).astype(np.float32)\n",
    "            S2 = src.read(4).astype(np.float32)\n",
    "            \n",
    "        idx = spyndex.computeIndex(\n",
    "            index=[\"NBRSWIR\"],\n",
    "            params={\n",
    "                \"S1\": S1,\n",
    "                \"S2\": S2\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return idx\n",
    "\n",
    "    def read_mask(self, path):\n",
    "        with rasterio.open(path) as src:\n",
    "            mask = src.read(1).astype(np.float32)\n",
    "            mask = np.nan_to_num(mask, nan=0.0)\n",
    "            mask = np.where(mask > 0, 1.0, 0.0)\n",
    "        return torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id_ = self.ids[idx]\n",
    "        fname = f\"recorte_{id_}.tif\"\n",
    "        T10_path = os.path.join(self.T10_dir, fname)\n",
    "        T20_path = os.path.join(self.T20_dir, fname)\n",
    "        mask_path = os.path.join(self.mask_dir, fname)\n",
    "\n",
    "        t1 = self.read_image(T10_path)\n",
    "        t2 = self.read_image(T20_path)\n",
    "        \n",
    "        idx = t1 - t2 \n",
    "        idx = np.expand_dims(idx, axis=0)\n",
    "\n",
    "        mask = self.read_mask(mask_path)\n",
    "\n",
    "        if self.transform:\n",
    "            idx = self.transform(torch.tensor(idx, dtype=torch.float32))\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        T = torch.tensor(idx, dtype=torch.float32)\n",
    "        return T, mask, id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f918fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, criterion, valid_loader, device, e):\n",
    "    model.eval()\n",
    "    valid_sum = 0\n",
    "    for j, batch in enumerate(valid_loader):\n",
    "        img, label = batch[0].float(), batch[1].float()\n",
    "        img, label = img.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img)\n",
    "            loss = criterion(outputs, label)\n",
    "        valid_sum += loss.item()\n",
    "        print('Epoch {:<3d}  |Step {:>3d}/{:<3d}  | valid loss {:.4f}'.format(e, j, len(valid_loader), loss.item()))\n",
    "\n",
    "    return valid_sum / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07c72cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "limiar_list = [0.05 * i for i in range(1, 10)]\n",
    "layers_list = [8, 32, 64]\n",
    "channels_list = [8, 1]\n",
    "load_num = 100\n",
    "channels = 8\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(0)\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.enabled = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aea6655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "split_df = pd.read_csv('split_ids.csv')\n",
    "val_ids = split_df[split_df['split'] == 'val']['ID'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b84a3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dice(labels, preds):\n",
    "    labels_flat = labels.flatten()\n",
    "    preds_flat = preds.flatten()\n",
    "    \n",
    "    labels_flat = labels_flat.astype(bool).astype(int)\n",
    "    preds_flat = preds_flat.astype(bool).astype(int)\n",
    "    \n",
    "    intersection = np.sum(labels_flat * preds_flat)\n",
    "    union = np.sum(labels_flat) + np.sum(preds_flat)\n",
    "    \n",
    "    if union == 0:\n",
    "        return 1.0 # sem labels em ambos, ou seja, predição perfeita\n",
    "    else:\n",
    "        return (2. * intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64b14c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for layers=8, threshold=0.05, channels=8\n",
      "Evaluating for layers=8, threshold=0.05, channels=1\n",
      "Evaluating for layers=8, threshold=0.10, channels=8\n",
      "Evaluating for layers=8, threshold=0.10, channels=1\n",
      "Evaluating for layers=8, threshold=0.15, channels=8\n",
      "Evaluating for layers=8, threshold=0.15, channels=1\n",
      "Evaluating for layers=8, threshold=0.20, channels=8\n",
      "Evaluating for layers=8, threshold=0.20, channels=1\n",
      "Evaluating for layers=8, threshold=0.25, channels=8\n",
      "Evaluating for layers=8, threshold=0.25, channels=1\n",
      "Evaluating for layers=8, threshold=0.30, channels=8\n",
      "Evaluating for layers=8, threshold=0.30, channels=1\n",
      "Evaluating for layers=8, threshold=0.35, channels=8\n",
      "Evaluating for layers=8, threshold=0.35, channels=1\n",
      "Evaluating for layers=8, threshold=0.40, channels=8\n",
      "Evaluating for layers=8, threshold=0.40, channels=1\n",
      "Evaluating for layers=8, threshold=0.45, channels=8\n",
      "Evaluating for layers=8, threshold=0.45, channels=1\n",
      "Evaluating for layers=32, threshold=0.05, channels=8\n",
      "Evaluating for layers=32, threshold=0.05, channels=1\n",
      "Evaluating for layers=32, threshold=0.10, channels=8\n",
      "Evaluating for layers=32, threshold=0.10, channels=1\n",
      "Evaluating for layers=32, threshold=0.15, channels=8\n",
      "Evaluating for layers=32, threshold=0.15, channels=1\n",
      "Evaluating for layers=32, threshold=0.20, channels=8\n",
      "Evaluating for layers=32, threshold=0.20, channels=1\n",
      "Evaluating for layers=32, threshold=0.25, channels=8\n",
      "Evaluating for layers=32, threshold=0.25, channels=1\n",
      "Evaluating for layers=32, threshold=0.30, channels=8\n",
      "Evaluating for layers=32, threshold=0.30, channels=1\n",
      "Evaluating for layers=32, threshold=0.35, channels=8\n",
      "Evaluating for layers=32, threshold=0.35, channels=1\n",
      "Evaluating for layers=32, threshold=0.40, channels=8\n",
      "Evaluating for layers=32, threshold=0.40, channels=1\n",
      "Evaluating for layers=32, threshold=0.45, channels=8\n",
      "Evaluating for layers=32, threshold=0.45, channels=1\n",
      "Evaluating for layers=64, threshold=0.05, channels=8\n",
      "Evaluating for layers=64, threshold=0.10, channels=8\n",
      "Evaluating for layers=64, threshold=0.15, channels=8\n",
      "Evaluating for layers=64, threshold=0.20, channels=8\n",
      "Evaluating for layers=64, threshold=0.25, channels=8\n",
      "Evaluating for layers=64, threshold=0.30, channels=8\n",
      "Evaluating for layers=64, threshold=0.35, channels=8\n",
      "Evaluating for layers=64, threshold=0.40, channels=8\n",
      "Evaluating for layers=64, threshold=0.45, channels=8\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dice_results = {}\n",
    "\n",
    "for layers in layers_list:\n",
    "    for l in limiar_list:\n",
    "        for channels in channels_list:\n",
    "            if channels == 1 and layers == 64:\n",
    "                continue\n",
    "            print(f'Evaluating for layers={layers}, threshold={l:.2f}, channels={channels}')\n",
    "            col_name = f'layers{layers}_limiar{l:.2f}_{channels}ch'\n",
    "            dice_results[col_name] = []\n",
    "            result_path = os.path.abspath('.') + '/results'\n",
    "            model_save_path = os.path.join(result_path, f'FCN_2D_{channels}ch_{layers}lyr')\n",
    "\n",
    "            net = FCN_2D(channels, layers).to(device)\n",
    "            net.load_state_dict(torch.load(model_save_path + '/net_%d.pkl' % load_num))\n",
    "            net.eval()\n",
    "\n",
    "            if channels == 8:\n",
    "                dataset_val = WorCapDataset(data_paths[\"T1\"], data_paths[\"T2\"], data_paths['mask'], val_ids)\n",
    "            elif channels == 1:\n",
    "                dataset_val = WorCapDiffDataset(data_paths[\"T1\"], data_paths[\"T2\"], data_paths['mask'], val_ids)\n",
    "            test_loader = DataLoader(dataset_val, batch_size=1, shuffle=False)\n",
    "\n",
    "            for j, batch in enumerate(test_loader):\n",
    "                img, label, id = batch[0].float(), batch[1].float(), str(batch[2].item())\n",
    "                img, label = img.to(device), label.to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = net(img)\n",
    "                outputs = torch.sigmoid(outputs)\n",
    "                outputs = outputs.squeeze(1).cpu().numpy()\n",
    "                label_np = label.squeeze(1).cpu().numpy()\n",
    "\n",
    "                pred_bin = (outputs >= l).astype(np.uint8)\n",
    "                label_bin = label_np.astype(np.uint8)\n",
    "\n",
    "                dice = calc_dice(label_bin, pred_bin)\n",
    "                dice_results[col_name].append(dice)\n",
    "\n",
    "df_dice = pd.DataFrame(dice_results)\n",
    "df_dice.index = val_ids\n",
    "df_dice.to_csv('dice_results.csv', index_label='ID')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8302652",
   "metadata": {},
   "outputs": [],
   "source": [
    "describr_df = df_dice.describe()\n",
    "\n",
    "top5_mean_cols = describr_df.loc['mean'].sort_values(ascending=False).head(5)\n",
    "top5_sec_qart_cols = describr_df.loc['50%'].sort_values(ascending=False).head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "424e4964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layers32_limiar0.15_8ch    0.821957\n",
       "layers32_limiar0.20_8ch    0.821328\n",
       "layers32_limiar0.10_8ch    0.821246\n",
       "layers32_limiar0.25_8ch    0.820496\n",
       "layers32_limiar0.30_8ch    0.820321\n",
       "Name: mean, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_mean_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7e12bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layers32_limiar0.25_8ch    0.932297\n",
       "layers32_limiar0.30_8ch    0.932197\n",
       "layers32_limiar0.20_8ch    0.929527\n",
       "layers32_limiar0.15_8ch    0.928064\n",
       "layers32_limiar0.10_8ch    0.927152\n",
       "Name: 50%, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_sec_qart_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2c24da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "limiar_selected = [0.15, 0.2, 0.25, 0.1, 0.3, 0.4, 0.5, 0.6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
